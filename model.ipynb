{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#\n",
    "\n",
    "class TextAnalysis:\n",
    "    def __init__(self, data_path, scores_path, train_dataset_path):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.data_path = data_path\n",
    "        self.scores_path = scores_path\n",
    "        self.train_dataset_path = train_dataset_path\n",
    "        self.df, self.code2prompts = self._load_and_process_data()\n",
    "        self.labeled_data_df, self.clf, self.vectorizer = self._train_naive_bayes_classifier()\n",
    "        self.models, self.evaluation = self._train_models_per_question()\n",
    "\n",
    "    def _remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
    "\n",
    "    def _load_and_process_data(self, data_path=None):\n",
    "        code2convos = dict()\n",
    "\n",
    "        pbar = tqdm.tqdm(sorted(self.data_path))\n",
    "        for path in pbar:\n",
    "            # print(Path.cwd() / path)\n",
    "            file_code = os.path.basename(path).split(\".\")[0]\n",
    "            with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "                    \n",
    "                # get the file id to use it as key later on\n",
    "                fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "                # read the html file\n",
    "                html_page = fh.read()\n",
    "\n",
    "                # parse the html file with bs4 so we can extract needed stuff\n",
    "                soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "                # grab the conversations with the data-testid pattern\n",
    "                data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "                conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "                convo_texts = []\n",
    "\n",
    "                for i, convo in enumerate(conversations):\n",
    "                    convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "                    if len(convo) > 0:\n",
    "                        role = convo[0].get(\"data-message-author-role\")\n",
    "                        convo_texts.append({\n",
    "                                \"role\" : role,\n",
    "                                \"text\" : convo[0].text\n",
    "                            }\n",
    "                        )\n",
    "                        \n",
    "                code2convos[file_code] = convo_texts\n",
    "\n",
    "        prompts = []\n",
    "        answers = []\n",
    "        code2prompts = dict()\n",
    "        code2answers = defaultdict(list)\n",
    "        for code , convos in code2convos.items():\n",
    "            user_prompts = []\n",
    "            for conv in convos:\n",
    "                if conv[\"role\"] == \"user\":\n",
    "                    prompts.append(conv[\"text\"].lower())\n",
    "                    user_prompts.append(conv[\"text\"].lower()) # Adding the lower case version of the prompt\n",
    "                else:\n",
    "                    answers.append(conv[\"text\"].lower())\n",
    "                    code2answers[code].append(conv[\"text\"].lower()) # Adding the lower case version of the answer\n",
    "\n",
    "            code2prompts[code] = user_prompts\n",
    "\n",
    "\n",
    "        # mapping prompts to answers\n",
    "        code2prompt_answer_pairs = defaultdict(list)\n",
    "\n",
    "        for code in code2convos:\n",
    "            for prompt, answer in zip(code2prompts[code], code2answers[code]):\n",
    "                code2prompt_answer_pairs[code].append((prompt, answer))\n",
    "\n",
    "\n",
    "\n",
    "        # Converting the dictionary to a DataFrame\n",
    "        refactored_data = []\n",
    "        for code, pairs in code2prompt_answer_pairs.items():\n",
    "            vectorized_pairs = [(prompt.split(), answer.split()) for prompt, answer in pairs]\n",
    "            refactored_data.append({'code': code, 'prompt_answer_pairs': vectorized_pairs})\n",
    "\n",
    "        df = pd.DataFrame(refactored_data)\n",
    "\n",
    "        scores = self.give_codes2scores()\n",
    "        \n",
    "\n",
    "        # join the scores with the df\n",
    "        df = df.merge(scores, on=\"code\")\n",
    "        df = df.sort_values(by=[\"grade\"], ascending=False)\n",
    "\n",
    "\n",
    "        return df, code2prompts\n",
    "    \n",
    "    def give_codes2scores(self):\n",
    "        # reading the scores\n",
    "        scores = pd.read_csv(self.scores_path, sep=\",\")\n",
    "        scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "        scores = scores[[\"code\", \"grade\"]]\n",
    "        #drop na\n",
    "        scores.dropna(inplace=True)\n",
    "        return scores\n",
    "\n",
    "    def _train_naive_bayes_classifier(self):\n",
    "        labeled_data_df = pd.read_csv(self.train_dataset_path, sep=\"\\t\")\n",
    "        labeled_data_df['prompt'] = labeled_data_df['prompt'].str.lower()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            labeled_data_df['prompt'],\n",
    "            labeled_data_df['related_question'],\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "\n",
    "        clf = MultinomialNB()\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        return labeled_data_df, clf, vectorizer\n",
    "    \n",
    "    def _train_models_per_question(self):\n",
    "        new_df = pd.DataFrame(columns=[\"prompt\", \"which_question\", \"grade\"])\n",
    "\n",
    "        for row in self.df.itertuples():\n",
    "            for prompt, answer in row.prompt_answer_pairs:\n",
    "                #removing stop words\n",
    "                prompt = self._remove_stopwords(prompt)\n",
    "                #convert prompt to string\n",
    "                promptStr = \" \".join(prompt)\n",
    "                #add it to a new a new df without using append\n",
    "                qNo = self._predict_question_number(promptStr)\n",
    "\n",
    "                new_df.loc[len(new_df.index)] = [promptStr, qNo, row.grade]\n",
    "\n",
    "\n",
    "        # replace Nan values with the mean of the column for column grade\n",
    "        new_df[\"grade\"].fillna((new_df[\"grade\"].mean()), inplace=True)\n",
    "        self.df = new_df.copy()\n",
    "\n",
    "\n",
    "        models = {}\n",
    "        evaluation = {}\n",
    "        for question_number in new_df['which_question'].unique():\n",
    "            question_data = new_df[new_df['which_question'] == question_number]\n",
    "            X = question_data['prompt']\n",
    "            y = question_data['grade']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            pipeline = make_pipeline(TfidfVectorizer(), LinearRegression())\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            models[question_number] = pipeline\n",
    "            evaluation[question_number] = {'MSE': mse, 'R2': r2}\n",
    "\n",
    "        return models, evaluation\n",
    "    \n",
    "    def _predict_question_number(self, prompt):\n",
    "        prompt_vect = self.vectorizer.transform([prompt.lower()])\n",
    "        return self.clf.predict(prompt_vect)[0]\n",
    "    \n",
    "\n",
    "    def predict_with_similarity_adjustment(self, prompt):\n",
    "        prompt = self._remove_stopwords(prompt.split())\n",
    "        prompt = \" \".join(prompt)\n",
    "        question_number = self._predict_question_number(prompt)\n",
    "        pipeline = self.models.get(question_number)\n",
    "        if not pipeline:\n",
    "            raise ValueError(f\"No model found for question number {question_number}.\")\n",
    "        vectorizer = pipeline.named_steps['tfidfvectorizer']\n",
    "        model = pipeline.named_steps['linearregression']\n",
    "        prompt_vector = vectorizer.transform([prompt])\n",
    "        train_vectors = vectorizer.transform(self.df[self.df['which_question'] == question_number]['prompt'])\n",
    "        similarities = cosine_similarity(prompt_vector, train_vectors)\n",
    "        max_similarity = np.max(similarities)\n",
    "        predicted_score = model.predict(prompt_vector)[0]\n",
    "        adjusted_score = predicted_score\n",
    "        return min(adjusted_score,100), max_similarity\n",
    "    \n",
    "    def predict_grades_for_multiple_prompts(self, code, prompts):\n",
    "        question_weights = {\n",
    "            'Q1': 0.05, 'Q2': 0.15, 'Q3': 0.05, 'Q4': 0.1,\n",
    "            'Q5': 0.2, 'Q6': 0.15, 'Q7': 0.2, 'Q8': 0.1\n",
    "        }\n",
    "\n",
    "        question_scores = {\n",
    "            'Q1': [], 'Q2': [], 'Q3': [], 'Q4': [], 'Q5': [], 'Q6': [], 'Q7': [], 'Q8': [],\n",
    "        }\n",
    "\n",
    "        # Predict and store scores for each prompt\n",
    "        for prompt in prompts:\n",
    "            score, _ = self.predict_with_similarity_adjustment(prompt)\n",
    "            numeric_question_number = self._predict_question_number(prompt)\n",
    "            question_number = f'Q{numeric_question_number}'\n",
    "            question_scores[question_number].append(score)\n",
    "\n",
    "        \n",
    "\n",
    "        total_weighted_score = 0\n",
    "        total_weight =0\n",
    "\n",
    "        noPromptForQuestionCounter = 0\n",
    "\n",
    "        # Calculate weighted score for each question\n",
    "        for question, scores in question_scores.items():\n",
    "            if scores:\n",
    "                average_score = sum(scores) / len(scores)\n",
    "                question_weight = question_weights[question]\n",
    "                total_weight += question_weight\n",
    "                weighted_score = average_score * question_weight\n",
    "                total_weighted_score += weighted_score\n",
    "            else:\n",
    "                noPromptForQuestionCounter += 1\n",
    "                #print(f\"No prompt for question {question}.\")\n",
    "                # Calculate average of other questions if no prompt for this question\n",
    "                #average_score = sum([sum(q_scores) / len(q_scores) for q_scores in question_scores.values() if (len(q_scores) > 1 )]) / len([q_scores for q_scores in question_scores.values() if (len(q_scores) > 1 )])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        if total_weight == 0:\n",
    "            print(\"HTML PAGE IS 404 FOR THE HTML CODE: \", code)\n",
    "            return -1\n",
    "        else:\n",
    "        #if (len(prompts) > 0 and noPromptForQuestionCounter < 10):\n",
    "            unweighted_score = total_weighted_score / total_weight\n",
    "            total_weighted_score += (1-total_weight) * 100\n",
    "\n",
    "            \n",
    "\n",
    "        return total_weighted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BONUS PART:\n",
    "bonus_data_path = \"data/deneme/*.html\"\n",
    "\n",
    "bonus_code2convos = dict()\n",
    "\n",
    "\n",
    "\n",
    "pbar = tqdm.tqdm(sorted(list(glob(bonus_data_path))))\n",
    "for path in pbar:\n",
    "    # print(Path.cwd() / path)\n",
    "    file_code = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "            \n",
    "        # get the file id to use it as key later on\n",
    "        fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "        # read the html file\n",
    "        html_page = fh.read()\n",
    "\n",
    "        # parse the html file with bs4 so we can extract needed stuff\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "        # grab the conversations with the data-testid pattern\n",
    "        data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "        conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "        convo_texts = []\n",
    "\n",
    "        for i, convo in enumerate(conversations):\n",
    "            convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "            if len(convo) > 0:\n",
    "                role = convo[0].get(\"data-message-author-role\")\n",
    "                convo_texts.append({\n",
    "                        \"role\" : role,\n",
    "                        \"text\" : convo[0].text\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        bonus_code2convos[file_code] = convo_texts\n",
    "\n",
    "prompts = []\n",
    "answers = []\n",
    "bonus_code2prompts = dict()\n",
    "code2answers = defaultdict(list)\n",
    "for code , convos in bonus_code2convos.items():\n",
    "    user_prompts = []\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            prompts.append(conv[\"text\"].lower())\n",
    "            user_prompts.append(conv[\"text\"].lower()) # Adding the lower case version of the prompt\n",
    "        else:\n",
    "            answers.append(conv[\"text\"].lower())\n",
    "            code2answers[code].append(conv[\"text\"].lower()) # Adding the lower case version of the answer\n",
    "\n",
    "    bonus_code2prompts[code] = user_prompts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### predicting scores from prompts:\n",
    "deneme_predicted_scores = []\n",
    "deneme_actual_scores = []\n",
    "for code, prompts in bonus_code2prompts.items():\n",
    "    predicted_score = ta.predict_grades_for_multiple_prompts(code, prompts)\n",
    "    if(predicted_score != -1):\n",
    "        deneme_predicted_scores.append(predicted_score)\n",
    "        matching_row = code2scores[code2scores['code'] == code]\n",
    "        if not matching_row.empty:\n",
    "            actual_score = matching_row['grade'].iloc[0]\n",
    "            deneme_actual_scores.append(actual_score)\n",
    "        else:\n",
    "            print(f\"Code {code} not found in code2scores\")\n",
    "        print(\"Predicted: \", predicted_score, \" Actual: \", actual_score, \" Code: \", code)\n",
    "    else:\n",
    "        print(code+\",\")\n",
    "\n",
    "mse = mean_squared_error(deneme_actual_scores, deneme_predicted_scores)\n",
    "rmse = mean_squared_error(deneme_actual_scores, deneme_predicted_scores, squared=False)\n",
    "mae = mean_absolute_error(deneme_actual_scores, deneme_predicted_scores)\n",
    "r2 = r2_score(deneme_actual_scores, deneme_predicted_scores)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#CROSS VALIDATION\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_scores, rmse_scores, mae_scores, r2_scores = [], [], [], []\n",
    "#train_paths, test_paths = load_and_split_data(\"data/html/*.html\",0.2)\n",
    "all_html_paths = sorted(list(glob(\"data/html/*.html\")))\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(all_html_paths):\n",
    "    # Split data into training and testing for this fold\n",
    "    train_files = [all_html_paths[i] for i in train_index]\n",
    "    test_files = [all_html_paths[i] for i in test_index]\n",
    "\n",
    "    ta = TextAnalysis(train_files, \"data/scores.csv\", \"data/labeled_data/train_dataset.csv\")\n",
    "\n",
    "    pbar = tqdm.tqdm(sorted(test_files))\n",
    "    for path in pbar:\n",
    "        # print(Path.cwd() / path)\n",
    "        file_code = os.path.basename(path).split(\".\")[0]\n",
    "        with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "                \n",
    "            # get the file id to use it as key later on\n",
    "            fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "            # read the html file\n",
    "            html_page = fh.read()\n",
    "\n",
    "            # parse the html file with bs4 so we can extract needed stuff\n",
    "            soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "            # grab the conversations with the data-testid pattern\n",
    "            data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "            conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "            convo_texts = []\n",
    "\n",
    "            for i, convo in enumerate(conversations):\n",
    "                convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "                if len(convo) > 0:\n",
    "                    role = convo[0].get(\"data-message-author-role\")\n",
    "                    convo_texts.append({\n",
    "                            \"role\" : role,\n",
    "                            \"text\" : convo[0].text\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "            bonus_code2convos[file_code] = convo_texts\n",
    "\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    bonus_code2prompts = dict()\n",
    "    code2answers = defaultdict(list)\n",
    "    for code , convos in bonus_code2convos.items():\n",
    "        user_prompts = []\n",
    "        for conv in convos:\n",
    "            if conv[\"role\"] == \"user\":\n",
    "                prompts.append(conv[\"text\"].lower())\n",
    "                user_prompts.append(conv[\"text\"].lower()) # Adding the lower case version of the prompt\n",
    "            else:\n",
    "                answers.append(conv[\"text\"].lower())\n",
    "                code2answers[code].append(conv[\"text\"].lower()) # Adding the lower case version of the answer\n",
    "\n",
    "        bonus_code2prompts[code] = user_prompts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### predicting scores from prompts:\n",
    "    deneme_predicted_scores = []\n",
    "    deneme_actual_scores = []\n",
    "    for code, prompts in bonus_code2prompts.items():\n",
    "        predicted_score = ta.predict_grades_for_multiple_prompts(code, prompts)\n",
    "        if(predicted_score != -1):\n",
    "            matching_row = code2scores[code2scores['code'] == code]\n",
    "            if not matching_row.empty:\n",
    "                actual_score = matching_row['grade'].iloc[0]\n",
    "                deneme_actual_scores.append(actual_score)\n",
    "                deneme_predicted_scores.append(predicted_score)\n",
    "\n",
    "            else:\n",
    "                print(f\"Code {code} not found in code2scores\")\n",
    "            #print(\"Predicted: \", predicted_score, \" Actual: \", actual_score, \" Code: \", code)\n",
    "        else:\n",
    "            print(code+\",\")\n",
    "\n",
    "    mse = mean_squared_error(deneme_actual_scores, deneme_predicted_scores)\n",
    "    rmse = mean_squared_error(deneme_actual_scores, deneme_predicted_scores, squared=False)\n",
    "    mae = mean_absolute_error(deneme_actual_scores, deneme_predicted_scores)\n",
    "    r2 = r2_score(deneme_actual_scores, deneme_predicted_scores)\n",
    "\n",
    "    # Append the scores\n",
    "    mse_scores.append(mse)\n",
    "    rmse_scores.append(rmse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT THE RESULTS\n",
    "for i in range(0, len(mse_scores)):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_scores[i]}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse_scores[i]}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae_scores[i]}\")\n",
    "    print(f\"R-squared (R2): {r2_scores[i]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"Average MSE: \", sum(mse_scores) / len(mse_scores))\n",
    "print(\"Average RMSE: \", sum(rmse_scores) / len(rmse_scores))\n",
    "print(\"Average MAE: \", sum(mae_scores) / len(mae_scores))\n",
    "print(\"Average R2: \", sum(r2_scores) / len(r2_scores))\n",
    "print(\"-----------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
