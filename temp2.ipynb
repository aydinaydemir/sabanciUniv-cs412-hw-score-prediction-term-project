{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "class TextAnalysis:\n",
    "    def __init__(self, data_path, scores_path, train_dataset_path):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.data_path = data_path\n",
    "        self.scores_path = scores_path\n",
    "        self.train_dataset_path = train_dataset_path\n",
    "        self.df, self.code2convos = self._load_and_process_data()\n",
    "        self.labeled_data_df, self.clf, self.vectorizer = self._train_naive_bayes_classifier()\n",
    "        self.models, self.evaluation = self._train_models_per_question()\n",
    "\n",
    "    def _remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
    "\n",
    "    def _load_and_process_data(self):\n",
    "        code2convos = dict()\n",
    "\n",
    "        pbar = tqdm.tqdm(sorted(list(glob(self.data_path))))\n",
    "        for path in pbar:\n",
    "            # print(Path.cwd() / path)\n",
    "            file_code = os.path.basename(path).split(\".\")[0]\n",
    "            with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "                    \n",
    "                # get the file id to use it as key later on\n",
    "                fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "                # read the html file\n",
    "                html_page = fh.read()\n",
    "\n",
    "                # parse the html file with bs4 so we can extract needed stuff\n",
    "                soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "                # grab the conversations with the data-testid pattern\n",
    "                data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "                conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "                convo_texts = []\n",
    "\n",
    "                for i, convo in enumerate(conversations):\n",
    "                    convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "                    if len(convo) > 0:\n",
    "                        role = convo[0].get(\"data-message-author-role\")\n",
    "                        convo_texts.append({\n",
    "                                \"role\" : role,\n",
    "                                \"text\" : convo[0].text\n",
    "                            }\n",
    "                        )\n",
    "                        \n",
    "                code2convos[file_code] = convo_texts\n",
    "\n",
    "        prompts = []\n",
    "        answers = []\n",
    "        code2prompts = defaultdict(list)\n",
    "        code2answers = defaultdict(list)\n",
    "        for code , convos in code2convos.items():\n",
    "            user_prompts = []\n",
    "            for conv in convos:\n",
    "                if conv[\"role\"] == \"user\":\n",
    "                    prompts.append(conv[\"text\"].lower())\n",
    "                    user_prompts.append(conv[\"text\"].lower()) # Adding the lower case version of the prompt\n",
    "                else:\n",
    "                    answers.append(conv[\"text\"].lower())\n",
    "                    code2answers[code].append(conv[\"text\"].lower()) # Adding the lower case version of the answer\n",
    "\n",
    "            code2prompts[code] = user_prompts\n",
    "\n",
    "\n",
    "        # mapping prompts to answers\n",
    "        code2prompt_answer_pairs = defaultdict(list)\n",
    "\n",
    "        for code in code2convos:\n",
    "            for prompt, answer in zip(code2prompts[code], code2answers[code]):\n",
    "                code2prompt_answer_pairs[code].append((prompt, answer))\n",
    "\n",
    "\n",
    "        code2prompt_answer_pairs[\"0031c86e-81f4-4eef-9e0e-28037abf9883\"][0]\n",
    "\n",
    "        # Converting the dictionary to a DataFrame\n",
    "        refactored_data = []\n",
    "        for code, pairs in code2prompt_answer_pairs.items():\n",
    "            vectorized_pairs = [(prompt.split(), answer.split()) for prompt, answer in pairs]\n",
    "            refactored_data.append({'code': code, 'prompt_answer_pairs': vectorized_pairs})\n",
    "\n",
    "        df = pd.DataFrame(refactored_data)\n",
    "\n",
    "\n",
    "        # reading the scores\n",
    "        scores = pd.read_csv(\"data/scores.csv\", sep=\",\")\n",
    "        scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "\n",
    "        # selecting the columns we need and we care\n",
    "        scores = scores[[\"code\", \"grade\"]]\n",
    "\n",
    "        # join the scores with the df\n",
    "        df = df.merge(scores, on=\"code\")\n",
    "        df = df.sort_values(by=[\"grade\"], ascending=False)\n",
    "\n",
    "\n",
    "        return df, code2convos\n",
    "\n",
    "    def _train_naive_bayes_classifier(self):\n",
    "        labeled_data_df = pd.read_csv(self.train_dataset_path, sep=\"\\t\")\n",
    "        labeled_data_df['prompt'] = labeled_data_df['prompt'].str.lower()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            labeled_data_df['prompt'],\n",
    "            labeled_data_df['related_question'],\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "\n",
    "        clf = MultinomialNB()\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        return labeled_data_df, clf, vectorizer\n",
    "    \n",
    "    def _train_models_per_question(self):\n",
    "        new_df = pd.DataFrame(columns=[\"prompt\", \"which_question\", \"grade\"])\n",
    "\n",
    "        for row in self.df.itertuples():\n",
    "            for prompt, answer in row.prompt_answer_pairs:\n",
    "                #removing stop words\n",
    "                prompt = self._remove_stopwords(prompt)\n",
    "                #convert prompt to string\n",
    "                promptStr = \" \".join(prompt)\n",
    "                #add it to a new a new df without using append\n",
    "                qNo = self._predict_question_number(promptStr)\n",
    "\n",
    "                new_df.loc[len(new_df.index)] = [promptStr, qNo, row.grade]\n",
    "\n",
    "\n",
    "        # replace Nan values with the mean of the column for column grade\n",
    "        new_df[\"grade\"].fillna((new_df[\"grade\"].mean()), inplace=True)\n",
    "        self.df = new_df.copy()\n",
    "\n",
    "\n",
    "        models = {}\n",
    "        evaluation = {}\n",
    "        for question_number in new_df['which_question'].unique():\n",
    "            question_data = new_df[new_df['which_question'] == question_number]\n",
    "            X = question_data['prompt']\n",
    "            y = question_data['grade']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            pipeline = make_pipeline(TfidfVectorizer(), LinearRegression())\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            models[question_number] = pipeline\n",
    "            evaluation[question_number] = {'MSE': mse, 'R2': r2}\n",
    "\n",
    "        return models, evaluation\n",
    "    \n",
    "    def _predict_question_number(self, prompt):\n",
    "        prompt_vect = self.vectorizer.transform([prompt.lower()])\n",
    "        return self.clf.predict(prompt_vect)[0]\n",
    "    \n",
    "\n",
    "    def predict_with_similarity_adjustment(self, prompt):\n",
    "        prompt = self._remove_stopwords(prompt.split())\n",
    "        prompt = \" \".join(prompt)\n",
    "        question_number = self._predict_question_number(prompt)\n",
    "        print(f\"Predicted question number: {question_number}\")\n",
    "        pipeline = self.models.get(question_number)\n",
    "        if not pipeline:\n",
    "            raise ValueError(f\"No model found for question number {question_number}.\")\n",
    "        vectorizer = pipeline.named_steps['tfidfvectorizer']\n",
    "        model = pipeline.named_steps['linearregression']\n",
    "        prompt_vector = vectorizer.transform([prompt])\n",
    "        train_vectors = vectorizer.transform(self.df[self.df['which_question'] == question_number]['prompt'])\n",
    "        similarities = cosine_similarity(prompt_vector, train_vectors)\n",
    "        max_similarity = np.max(similarities)\n",
    "        predicted_score = model.predict(prompt_vector)[0]\n",
    "        adjusted_score = predicted_score * max_similarity\n",
    "        return adjusted_score, max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:05<00:00, 21.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ta = TextAnalysis(\"data/html/*.html\", \"data/scores.csv\", \"data/labeled_data/train_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted question number: 5\n",
      "Predicted Score: 90.92055817080906 Max Similarity: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "new_prompt = 'Tune Hyperparameters'\n",
    "predicted_score, max_similarity = ta.predict_with_similarity_adjustment(new_prompt)\n",
    "print(\"Predicted Score:\", predicted_score, \"Max Similarity:\", max_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
